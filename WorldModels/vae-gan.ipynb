{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from vae.vae import CVAE\n",
    "# from controller import make_controller\n",
    "from env import make_env\n",
    "from utils import PARSER\n",
    "args = PARSER.parse_args(['--config_path', 'configs/carracing.config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "IM_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "Dataset used is celebA : https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_gen():\n",
    "    dirname = 'results/{}/{}/record4'.format(args.exp_name, args.env_name)\n",
    "    filenames = os.listdir(dirname)[:10000] # only use first 10k episodes\n",
    "    n = len(filenames)\n",
    "    im = []\n",
    "    for j, fname in enumerate(filenames):\n",
    "        if not fname.endswith('npz'): \n",
    "            continue\n",
    "        file_path = os.path.join(dirname, fname)\n",
    "        with np.load(file_path) as data:\n",
    "            N = data['obs'].shape[0]\n",
    "            for i, img in enumerate(data['obs']):\n",
    "                img_i = img / 255.0\n",
    "                # im.append(img_i)\n",
    "                yield img_i \n",
    "# im_dataset = tf.data.Dataset.from_tensors(im)\n",
    "im_dataset = tf.data.Dataset.from_generator(ds_gen, output_types=tf.float32, output_shapes=(64, 64, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "len dataset :  -2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "im_dataset_repeated = im_dataset.repeat()\n",
    "im_dataset_batch = im_dataset_repeated.batch(batch_size)\n",
    "\n",
    "for x in im_dataset.take(1) :\n",
    "    print(x.numpy().shape)\n",
    "    print(\"len dataset : \", tf.data.experimental.cardinality(im_dataset).numpy())\n",
    "    plt.imshow(x)\n",
    "    \n",
    "batch_gen = iter(im_dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 32\n",
    "LATENT_DEPTH = 512\n",
    "K_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean, logsigma = args\n",
    "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mean))\n",
    "    return mean + tf.exp(logsigma / 2) * epsilon\n",
    "\n",
    "def encoder():\n",
    "    input_E = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*2, kernel_size=K_SIZE, strides=2, padding='same')(input_E)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(LATENT_DEPTH)(X)    \n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    mean = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    logsigma = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    latent = keras.layers.Lambda(sampling, output_shape=(LATENT_DEPTH,))([mean, logsigma])\n",
    "    \n",
    "    kl_loss = 1 + logsigma - keras.backend.square(mean) - keras.backend.exp(logsigma)\n",
    "    kl_loss = keras.backend.mean(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return keras.models.Model(input_E, [latent,kl_loss])\n",
    "\n",
    "def generator():\n",
    "    input_G = keras.layers.Input(shape=(LATENT_DEPTH,))\n",
    "\n",
    "    X = keras.layers.Dense(8*8*DEPTH*8)(input_G)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.Reshape((8, 8, DEPTH * 8))(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=3, kernel_size=K_SIZE, padding='same')(X)\n",
    "    X = keras.layers.Activation('sigmoid')(X)\n",
    "\n",
    "    return keras.models.Model(input_G, X)\n",
    "\n",
    "def discriminator():\n",
    "    input_D = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, padding='same')(X)\n",
    "    inner_output = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(DEPTH*8)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(X)    \n",
    "    \n",
    "    return keras.models.Model(input_D, [output, inner_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in im_dataset_batch.take(1) :\n",
    "    test_images = x\n",
    "test_r = tf.random.normal((batch_size, LATENT_DEPTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = encoder()\n",
    "G = generator()\n",
    "D = discriminator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 1010  gan_loss 1.755 vae_loss 0.173 fake_dis_loss 5.388 r_dis_loss 0.0 t_dis_loss 0.815 vae_inner_loss 0.014 E_loss 0.032 D_loss 1.755 kl_loss 0.047 normal_loss 0.1834"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1fd8fc37d822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_vaegan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "#lr=0.0001\n",
    "E_opt = keras.optimizers.Adam(lr=lr)\n",
    "G_opt = keras.optimizers.Adam(lr=lr)\n",
    "D_opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "inner_loss_coef = 1\n",
    "normal_coef = 0.1\n",
    "kl_coef = 0.01\n",
    "\n",
    "@tf.function\n",
    "def train_step_vaegan(x):\n",
    "    lattent_r =  tf.random.normal((batch_size, LATENT_DEPTH))\n",
    "    with tf.GradientTape(persistent=True) as tape :\n",
    "        lattent,kl_loss = E(x)\n",
    "        fake = G(lattent)\n",
    "        dis_fake,dis_inner_fake = D(fake)\n",
    "        dis_fake_r,_ = D(G(lattent_r))\n",
    "        dis_true,dis_inner_true = D(x)\n",
    "\n",
    "        vae_inner = dis_inner_fake-dis_inner_true\n",
    "        vae_inner = vae_inner*vae_inner\n",
    "        \n",
    "        mean,var = tf.nn.moments(E(x)[0], axes=0)\n",
    "        var_to_one = var - 1\n",
    "        \n",
    "        normal_loss = tf.reduce_mean(mean*mean) + tf.reduce_mean(var_to_one*var_to_one)\n",
    "        \n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        vae_diff_loss = tf.reduce_mean(vae_inner)\n",
    "        f_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake), dis_fake))\n",
    "        r_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake_r), dis_fake_r))\n",
    "        t_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(dis_true), dis_true))\n",
    "        gan_loss = (0.5*t_dis_loss + 0.25*f_dis_loss + 0.25*r_dis_loss)\n",
    "        vae_loss = tf.reduce_mean(tf.abs(x-fake)) \n",
    "        E_loss = vae_diff_loss + kl_coef*kl_loss + normal_coef*normal_loss\n",
    "        G_loss = inner_loss_coef*vae_diff_loss - gan_loss\n",
    "        D_loss = gan_loss\n",
    "    \n",
    "    E_grad = tape.gradient(E_loss,E.trainable_variables)\n",
    "    G_grad = tape.gradient(G_loss,G.trainable_variables)\n",
    "    D_grad = tape.gradient(D_loss,D.trainable_variables)\n",
    "    del tape\n",
    "    E_opt.apply_gradients(zip(E_grad, E.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, G.trainable_variables))\n",
    "    D_opt.apply_gradients(zip(D_grad, D.trainable_variables))\n",
    "\n",
    "    return [gan_loss, vae_loss, f_dis_loss, r_dis_loss, t_dis_loss, vae_diff_loss, E_loss, D_loss, kl_loss, normal_loss]\n",
    "\n",
    "step = 0\n",
    "max_step = 10000000\n",
    "log_freq,img_log_freq = 10, 100\n",
    "save_freq,save_number_mult = 1000, 10000\n",
    "\n",
    "metrics_names = [\"gan_loss\", \"vae_loss\", \"fake_dis_loss\", \"r_dis_loss\", \"t_dis_loss\", \"vae_inner_loss\", \"E_loss\", \"D_loss\", \"kl_loss\", \"normal_loss\"]\n",
    "metrics = []\n",
    "for m in metrics_names :\n",
    "    metrics.append(tf.keras.metrics.Mean('m', dtype=tf.float32))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = ('logs/sep_D%dL%d/' % (DEPTH,LATENT_DEPTH)) + current_time \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "name = ('sep_D%dL%d' % (DEPTH,LATENT_DEPTH))\n",
    "\n",
    "def save_model() :\n",
    "    nb = str(step // save_number_mult)\n",
    "    D.save('saved-models/D_training_' + nb + '.h5')\n",
    "    G.save('saved-models/G_training_' + nb + '.h5')\n",
    "    E.save('saved-models/E_training_' + nb + '.h5')\n",
    "\n",
    "def print_metrics() :\n",
    "    s = \"\"\n",
    "    for name,metric in zip(metrics_names,metrics) :\n",
    "        s+= \" \" + name + \" \" + str(np.around(metric.result().numpy(), 3)) \n",
    "    print(f\"\\rStep : \" + str(step) + \" \" + s, end=\"\", flush=True)\n",
    "    with train_summary_writer.as_default():\n",
    "        for name,metric in zip(metrics_names,metrics) :\n",
    "            tf.summary.scalar(name, metric.result(), step=step)\n",
    "    for metric in metrics : \n",
    "        metric.reset_states()\n",
    "\n",
    "        \n",
    "def log_images() :\n",
    "    with train_summary_writer.as_default():\n",
    "        lattent,_ = E(test_images)\n",
    "        fake = G(lattent)\n",
    "        fake_r = G(test_r)\n",
    "        tf.summary.image(\"reconstructed image\", fake[:8], step=step, max_outputs=8)\n",
    "        tf.summary.image(\"random image\", fake_r[:8], step=step, max_outputs=8)\n",
    "        dis_fake,inner_dis_fake = D(fake)\n",
    "        dis_fake_r,inner_dis_fake_r = D(fake_r)\n",
    "        dis_true,inner_dis_true = D(test_images)\n",
    "        tf.summary.histogram(\"dis fake\", inner_dis_fake, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis true\", inner_dis_true, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis random\", inner_dis_fake_r, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis lattent\", lattent, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis normal\", tf.random.normal((batch_size, LATENT_DEPTH)), step=step, buckets=20)        \n",
    "\n",
    "for x in batch_gen : #batch_gen \n",
    "    step += 1\n",
    "    if not step % log_freq :\n",
    "        print_metrics()\n",
    "    if not step % img_log_freq :\n",
    "        log_images()\n",
    "    if not step % save_freq :\n",
    "        save_model()\n",
    "    \n",
    "    results = train_step_vaegan(x)\n",
    "    for metric,result in zip(metrics, results) :\n",
    "        metric(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.summary.create_file_writer('test/logs')\n",
    "with w.as_default():\n",
    "    tf.summary.histogram(\"activations\", tf.random.uniform([100, 50]), step=0)\n",
    "    tf.summary.histogram(\"initial_weights\", tf.random.normal([1000]), step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 31968), started 0:01:14 ago. (Use '!kill 31968' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9abca9f55f503e1c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9abca9f55f503e1c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
