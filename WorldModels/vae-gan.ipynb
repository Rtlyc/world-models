{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.8-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.0-py3-none-any.whl (2.3 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.0-py2.py3-none-any.whl (146 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=3299c31bbe84dcff6853c940ebe4896f6aa7861a289de1d638045a7f6d61f0c3\n",
      "  Stored in directory: c:\\users\\abhis\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=cb96cfaf702d8a7c370a0fd6be53ab8f2552b3e6dc2cb2e5f165d157f364367c\n",
      "  Stored in directory: c:\\users\\abhis\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: wheel, gast, google-pasta, termcolor, astunparse, tensorflow-estimator, numpy, keras-preprocessing, absl-py, opt-einsum, flatbuffers, protobuf, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, markdown, grpcio, tensorboard-plugin-wit, tensorboard, wrapt, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.3.3 google-auth-1.30.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.0 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'controller'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a35db41d344b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from vae.vae import CVAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcontroller\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_controller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPARSER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'controller'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from vae.vae import CVAE\n",
    "from controller import make_controller\n",
    "from env import make_env\n",
    "from utils import PARSER\n",
    "args = PARSER.parse_args(['--config_path', 'configs/carracing.config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "Dataset used is celebA : https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ./data/img_align_celeba/img_align_celeba/*.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1cc4d55c716e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mim_patern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'./data/img_align_celeba/img_align_celeba/*.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mfiles_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_patern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mim_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"https://www.kaggle.com/jessicali9530/celeba-dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mim_dataset_repeated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[1;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[0;32m   1222\u001b[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0m\u001b[0;32m   1225\u001b[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0;32m   1226\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[0;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[0mdata_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_summarize_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[0;32m    155\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ./data/img_align_celeba/img_align_celeba/*.jpg'"
     ]
    }
   ],
   "source": [
    "IM_DIM = 64\n",
    "@tf.function\n",
    "def im_preprocessing(im_path) :\n",
    "    im_file = tf.io.read_file(im_path)\n",
    "    im = tf.io.decode_jpeg(im_file)\n",
    "    im = tf.image.convert_image_dtype(im, tf.float32)\n",
    "    #im = tf.image.crop_to_bounding_box(im, 20,0,178,178)\n",
    "    im = tf.image.resize(im, [IM_DIM, IM_DIM])\n",
    "    im = tf.image.random_flip_left_right(im)\n",
    "    return(im)\n",
    "\n",
    "batch_size = 32\n",
    "im_patern = r'./data/img_align_celeba/img_align_celeba/*.jpg'\n",
    "files_dataset = tf.data.Dataset.list_files(im_patern)\n",
    "im_dataset = files_dataset.map(im_preprocessing(r\"https://www.kaggle.com/jessicali9530/celeba-dataset\"))\n",
    "im_dataset_repeated = im_dataset.repeat()\n",
    "im_dataset_batch = im_dataset_repeated.batch(batch_size)\n",
    "\n",
    "for x in im_dataset.take(1) :\n",
    "    print(x.numpy().shape)\n",
    "    print(\"len dataset : \", tf.data.experimental.cardinality(im_dataset).numpy())\n",
    "    plt.imshow(x)\n",
    "    \n",
    "batch_gen = iter(im_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"results/{}/{}/record\".format(args.exp_name, args.env_name)\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "obs = np.load(os.path.join(DATA_DIR, random.choice(filelist)))[\"obs\"]\n",
    "obs = obs.astype(np.float32)/255.0\n",
    "obs.shape\n",
    "frame = random.choice(obs).reshape(1, 64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 32\n",
    "LATENT_DEPTH = 512\n",
    "K_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean, logsigma = args\n",
    "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mean))\n",
    "    return mean + tf.exp(logsigma / 2) * epsilon\n",
    "\n",
    "def encoder():\n",
    "    input_E = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*2, kernel_size=K_SIZE, strides=2, padding='same')(input_E)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(LATENT_DEPTH)(X)    \n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    mean = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    logsigma = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    latent = keras.layers.Lambda(sampling, output_shape=(LATENT_DEPTH,))([mean, logsigma])\n",
    "    \n",
    "    kl_loss = 1 + logsigma - keras.backend.square(mean) - keras.backend.exp(logsigma)\n",
    "    kl_loss = keras.backend.mean(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return keras.models.Model(input_E, [latent,kl_loss])\n",
    "\n",
    "def generator():\n",
    "    input_G = keras.layers.Input(shape=(LATENT_DEPTH,))\n",
    "\n",
    "    X = keras.layers.Dense(8*8*DEPTH*8)(input_G)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.Reshape((8, 8, DEPTH * 8))(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=3, kernel_size=K_SIZE, padding='same')(X)\n",
    "    X = keras.layers.Activation('sigmoid')(X)\n",
    "\n",
    "    return keras.models.Model(input_G, X)\n",
    "\n",
    "def discriminator():\n",
    "    input_D = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, padding='same')(X)\n",
    "    inner_output = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(DEPTH*8)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(X)    \n",
    "    \n",
    "    return keras.models.Model(input_D, [output, inner_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in im_dataset_batch.take(1) :\n",
    "    test_images = x\n",
    "test_r = tf.random.normal((batch_size, LATENT_DEPTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = encoder()\n",
    "G = generator()\n",
    "D = discriminator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 14900  gan_loss 0.171 vae_loss 0.182 fake_dis_loss 0.319 r_dis_loss 0.066 t_dis_loss 0.15 vae_inner_loss 0.021 E_loss 0.034 D_loss 0.171 kl_loss 0.16 normal_loss 0.10708"
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "#lr=0.0001\n",
    "E_opt = keras.optimizers.Adam(lr=lr)\n",
    "G_opt = keras.optimizers.Adam(lr=lr)\n",
    "D_opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "inner_loss_coef = 1\n",
    "normal_coef = 0.1\n",
    "kl_coef = 0.01\n",
    "\n",
    "@tf.function\n",
    "def train_step_vaegan(x):\n",
    "    lattent_r =  tf.random.normal((batch_size, LATENT_DEPTH))\n",
    "    with tf.GradientTape(persistent=True) as tape :\n",
    "        lattent,kl_loss = E(x)\n",
    "        fake = G(lattent)\n",
    "        dis_fake,dis_inner_fake = D(fake)\n",
    "        dis_fake_r,_ = D(G(lattent_r))\n",
    "        dis_true,dis_inner_true = D(x)\n",
    "\n",
    "        vae_inner = dis_inner_fake-dis_inner_true\n",
    "        vae_inner = vae_inner*vae_inner\n",
    "        \n",
    "        mean,var = tf.nn.moments(E(x)[0], axes=0)\n",
    "        var_to_one = var - 1\n",
    "        \n",
    "        normal_loss = tf.reduce_mean(mean*mean) + tf.reduce_mean(var_to_one*var_to_one)\n",
    "        \n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        vae_diff_loss = tf.reduce_mean(vae_inner)\n",
    "        f_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake), dis_fake))\n",
    "        r_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake_r), dis_fake_r))\n",
    "        t_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(dis_true), dis_true))\n",
    "        gan_loss = (0.5*t_dis_loss + 0.25*f_dis_loss + 0.25*r_dis_loss)\n",
    "        vae_loss = tf.reduce_mean(tf.abs(x-fake)) \n",
    "        E_loss = vae_diff_loss + kl_coef*kl_loss + normal_coef*normal_loss\n",
    "        G_loss = inner_loss_coef*vae_diff_loss - gan_loss\n",
    "        D_loss = gan_loss\n",
    "    \n",
    "    E_grad = tape.gradient(E_loss,E.trainable_variables)\n",
    "    G_grad = tape.gradient(G_loss,G.trainable_variables)\n",
    "    D_grad = tape.gradient(D_loss,D.trainable_variables)\n",
    "    del tape\n",
    "    E_opt.apply_gradients(zip(E_grad, E.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, G.trainable_variables))\n",
    "    D_opt.apply_gradients(zip(D_grad, D.trainable_variables))\n",
    "\n",
    "    return [gan_loss, vae_loss, f_dis_loss, r_dis_loss, t_dis_loss, vae_diff_loss, E_loss, D_loss, kl_loss, normal_loss]\n",
    "\n",
    "step = 0\n",
    "max_step = 10000000\n",
    "log_freq,img_log_freq = 10, 100\n",
    "save_freq,save_number_mult = 1000, 10000\n",
    "\n",
    "metrics_names = [\"gan_loss\", \"vae_loss\", \"fake_dis_loss\", \"r_dis_loss\", \"t_dis_loss\", \"vae_inner_loss\", \"E_loss\", \"D_loss\", \"kl_loss\", \"normal_loss\"]\n",
    "metrics = []\n",
    "for m in metrics_names :\n",
    "    metrics.append(tf.keras.metrics.Mean('m', dtype=tf.float32))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = ('logs/sep_D%dL%d/' % (DEPTH,LATENT_DEPTH)) + current_time \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "name = ('sep_D%dL%d' % (DEPTH,LATENT_DEPTH))\n",
    "\n",
    "def save_model() :\n",
    "    nb = str(step // save_number_mult)\n",
    "    D.save('saved-models/D_training_' + nb + '.h5')\n",
    "    G.save('saved-models/G_training_' + nb + '.h5')\n",
    "    E.save('saved-models/E_training_' + nb + '.h5')\n",
    "\n",
    "def print_metrics() :\n",
    "    s = \"\"\n",
    "    for name,metric in zip(metrics_names,metrics) :\n",
    "        s+= \" \" + name + \" \" + str(np.around(metric.result().numpy(), 3)) \n",
    "    print(f\"\\rStep : \" + str(step) + \" \" + s, end=\"\", flush=True)\n",
    "    with train_summary_writer.as_default():\n",
    "        for name,metric in zip(metrics_names,metrics) :\n",
    "            tf.summary.scalar(name, metric.result(), step=step)\n",
    "    for metric in metrics : \n",
    "        metric.reset_states()\n",
    "\n",
    "        \n",
    "def log_images() :\n",
    "    with train_summary_writer.as_default():\n",
    "        lattent,_ = E(test_images)\n",
    "        fake = G(lattent)\n",
    "        fake_r = G(test_r)\n",
    "        tf.summary.image(\"reconstructed image\", fake[:8], step=step, max_outputs=8)\n",
    "        tf.summary.image(\"random image\", fake_r[:8], step=step, max_outputs=8)\n",
    "        dis_fake,inner_dis_fake = D(fake)\n",
    "        dis_fake_r,inner_dis_fake_r = D(fake_r)\n",
    "        dis_true,inner_dis_true = D(test_images)\n",
    "        tf.summary.histogram(\"dis fake\", inner_dis_fake, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis true\", inner_dis_true, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis random\", inner_dis_fake_r, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis lattent\", lattent, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis normal\", tf.random.normal((batch_size, LATENT_DEPTH)), step=step, buckets=20)        \n",
    "\n",
    "for x in frame : #batch_gen \n",
    "    step += 1\n",
    "    if not step % log_freq :\n",
    "        print_metrics()\n",
    "    if not step % img_log_freq :\n",
    "        log_images()\n",
    "    if not step % save_freq :\n",
    "        save_model()\n",
    "    \n",
    "    results = train_step_vaegan(x)\n",
    "    for metric,result in zip(metrics, results) :\n",
    "        metric(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
